import streamlit as st
from modules.document_processor import load_all_documents
from modules.vector_store import prepare_vectorstore
from langchain.chains import RetrievalQA
from langchain.chains.llm import LLMChain
from langchain_openai import OpenAI, AzureOpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import StuffDocumentsChain
import os
from dotenv import load_dotenv

# Kh·ªüi t·∫°o b·ªô nh·ªõ
def init_memory():
    return ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )

# T·∫°o prompt c√¢u h·ªèi m·ªü r·ªông
follow_up_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    B·∫°n l√† tr·ª£ l√Ω AI. D·ª±a tr√™n t√†i li·ªáu v√† h·ªôi tho·∫°i tr∆∞·ªõc, h√£y ƒë·∫∑t m·ªôt c√¢u h·ªèi m·ªü r·ªông:

    T√†i li·ªáu:
    {context}

    L·ªãch s·ª≠ h·ªôi tho·∫°i:
    {chat_history}

    H√£y ƒë·∫∑t m·ªôt c√¢u h·ªèi m·ªü r·ªông:
    """
)

def build_rag_chain(vectorstore):
    load_dotenv()
    api_key = os.getenv("GITHUB_TOKEN")

    try:
        llm = AzureOpenAI(
            api_key=api_key,
            api_version="2023-05-15",
            azure_endpoint="https://models.inference.ai.azure.com",
            deployment_name="gpt-4o",
            temperature=0.7
        )
    except Exception:
        llm = OpenAI(
            api_key=api_key,
            base_url="https://models.inference.ai.azure.com",
            model_name="gpt-4o",
            temperature=0.7
        )

    memory = init_memory()

    qa_prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
        S·ª≠ d·ª•ng th√¥ng tin sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu kh√¥ng bi·∫øt th√¨ n√≥i kh√¥ng bi·∫øt.

        T√†i li·ªáu: {context}
        C√¢u h·ªèi: {question}
        Tr·∫£ l·ªùi:
        """
    )

    llm_chain = LLMChain(llm=llm, prompt=qa_prompt)

    # ‚úÖ B·ªçc v√†o StuffDocumentsChain
    combine_documents_chain = StuffDocumentsChain(
        llm_chain=llm_chain,
        document_variable_name="context"  # << kh·ªõp v·ªõi t√™n trong PromptTemplate
    )

    # ‚úÖ D√πng RetrievalQA th·ªß c√¥ng
    main_chain = RetrievalQA(
        retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
        combine_documents_chain=combine_documents_chain,
        return_source_documents=True,
        memory=memory,
        input_key="question"  # R·∫•t quan tr·ªçng!
    )

    # ‚úÖ Follow-up chain
    follow_up_chain = LLMChain(
        llm=llm,
        prompt=follow_up_prompt,
        output_key="text"
    )

    return main_chain, follow_up_chain, memory


def main():
    st.title("ü§ñ AI Agent Th√¥ng Minh")

    if "conversation" not in st.session_state:
        with st.spinner("üîÑ ƒêang kh·ªüi t·∫°o..."):
            try:
                documents = load_all_documents("data/")
                vectordb = prepare_vectorstore(documents)
                st.session_state.main_chain, st.session_state.follow_up_chain, st.session_state.memory = build_rag_chain(vectordb)
                st.session_state.conversation = []
                st.success("‚úÖ S·∫µn s√†ng!")
            except Exception as e:
                st.error(f"L·ªói kh·ªüi t·∫°o: {str(e)}")
                st.exception(e)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠
    for msg in st.session_state.conversation:
        role = "üë§ Ng∆∞·ªùi d√πng" if msg["role"] == "user" else "ü§ñ AI"
        st.markdown(f"**{role}:** {msg['content']}")

    query = st.text_input("üí¨ Nh·∫≠p c√¢u h·ªèi ho·∫∑c g√µ 'ti·∫øp' ƒë·ªÉ nh·∫≠n c√¢u h·ªèi m·ªü r·ªông:")

    if query:
        if query.lower() == "ti·∫øp":
            with st.spinner("ü§ñ ƒêang ƒë·∫∑t c√¢u h·ªèi m·ªü r·ªông..."):
                chat_history_text = "\n".join([
                    f"Ng∆∞·ªùi d√πng: {m['content']}" if m["role"] == "user" else f"AI: {m['content']}"
                    for m in st.session_state.conversation
                ])
                try:
                    docs = st.session_state.main_chain.retriever.get_relevant_documents("")
                    context_text = "\n".join([doc.page_content for doc in docs[:3]])

                    follow_up_response = st.session_state.follow_up_chain.invoke({
                        "context": context_text,
                        "chat_history": chat_history_text
                    })

                    follow_up_text = follow_up_response.get("text", "B·∫°n mu·ªën bi·∫øt th√™m v·ªÅ ch·ªß ƒë·ªÅ n√†y?")
                    st.session_state.conversation.append({"role": "ai", "content": follow_up_text})
                    st.experimental_rerun()
                except Exception as e:
                    st.error(f"L·ªói khi t·∫°o c√¢u h·ªèi m·ªü r·ªông: {str(e)}")
                    st.exception(e)
        else:
            with st.spinner("ü§ñ ƒêang tr·∫£ l·ªùi..."):
                try:
                    st.session_state.memory.chat_memory.add_user_message(query)

                    response = st.session_state.main_chain({"question": query})

                    st.session_state.conversation.append({"role": "user", "content": query})
                    answer_content = response.get('result', response.get('answer', "Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi"))

                    st.session_state.memory.chat_memory.add_ai_message(answer_content)

                    source_docs = response.get('source_documents', [])
                    sources = [doc.metadata.get('filename', '') for doc in source_docs]

                    st.session_state.conversation.append({
                        "role": "ai",
                        "content": f"{answer_content}\n\nüîó Ngu·ªìn: {sources}"
                    })

                    chat_history_text = "\n".join([
                        f"Ng∆∞·ªùi d√πng: {m['content']}" if m["role"] == "user" else f"AI: {m['content']}"
                        for m in st.session_state.conversation
                    ])
                    context_text = "\n".join([doc.page_content for doc in source_docs[:3]])

                    follow_up_response = st.session_state.follow_up_chain.invoke({
                        "context": context_text,
                        "chat_history": chat_history_text
                    })

                    follow_up_text = follow_up_response.get("text", "B·∫°n mu·ªën bi·∫øt th√™m v·ªÅ ch·ªß ƒë·ªÅ n√†y?")
                    st.session_state.conversation.append({
                        "role": "ai",
                        "content": f"üí° B·∫°n c√≥ mu·ªën h·ªèi ti·∫øp v·ªÅ: '{follow_up_text}'? (G√µ 'ti·∫øp' ƒë·ªÉ xem)"
                    })
                    st.experimental_rerun()
                except Exception as e:
                    st.error(f"L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")
                    st.exception(e)

if __name__ == "__main__":
    main()
