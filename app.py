import streamlit as st
from modules.document_processor import load_all_documents
from modules.vector_store import prepare_vectorstore
from langchain.chains import RetrievalQA
from langchain.chains.llm import LLMChain
from langchain_openai import OpenAI, AzureOpenAI
from langchain.chat_models import ChatOpenAI 
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import StuffDocumentsChain
from langchain_community.llms import HuggingFaceHub
from langchain_community.llms import Ollama
import os
from dotenv import load_dotenv

# Kh·ªüi t·∫°o b·ªô nh·ªõ
def init_memory():
    return ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="result"  # Th√™m output_key ƒë·ªÉ ch·ªâ ƒë·ªãnh r√µ kh√≥a c·∫ßn l∆∞u
    )

# T·∫°o prompt c√¢u h·ªèi m·ªü r·ªông
follow_up_prompt = PromptTemplate(
    input_variables=["context", "question", "chat_history"],
    template="""
    B·∫°n l√† tr·ª£ l√Ω AI. D·ª±a tr√™n t√†i li·ªáu v√† h·ªôi tho·∫°i tr∆∞·ªõc, h√£y ƒë·∫∑t m·ªôt c√¢u h·ªèi m·ªü r·ªông:

    T√†i li·ªáu:
    {context}

    L·ªãch s·ª≠ h·ªôi tho·∫°i:
    {chat_history}

    H√£y ƒë·∫∑t m·ªôt c√¢u h·ªèi m·ªü r·ªông:
    """
)

def build_rag_chain(vectorstore):
    load_dotenv()
    github_token = os.getenv("GITHUB_TOKEN")
    huggingface_token = os.getenv("HUGGINGFACE_TOKEN")
    
    llm = ChatOpenAI(
        openai_api_base="https://models.inference.ai.azure.com",
        openai_api_key=github_token,
        model="gpt-4o",
        temperature=0.3
    )
    
    # llm = HuggingFaceHub(repo_id="mistralai/Mistral-7B-Instruct-v0.1", huggingfacehub_api_token= huggingface_token)

    memory = init_memory()

    qa_prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
        S·ª≠ d·ª•ng th√¥ng tin sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu kh√¥ng bi·∫øt th√¨ n√≥i kh√¥ng bi·∫øt.

        T√†i li·ªáu: {context}
        C√¢u h·ªèi: {question}
        Tr·∫£ l·ªùi:
        """
    )

    llm_chain = LLMChain(llm=llm, prompt=qa_prompt)

    # ‚úÖ B·ªçc v√†o StuffDocumentsChain
    combine_documents_chain = StuffDocumentsChain(
        llm_chain=llm_chain,
        document_variable_name="context"  # << kh·ªõp v·ªõi t√™n trong PromptTemplate
    )

    # ‚úÖ D√πng RetrievalQA th·ªß c√¥ng
    main_chain = RetrievalQA(
        retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
        combine_documents_chain=combine_documents_chain,
        return_source_documents=True,
        memory=memory,
        input_key="question",  # R·∫•t quan tr·ªçng!
        output_key="result"    # Ch·ªâ ƒë·ªãnh output_key ƒë·ªÉ kh·ªõp v·ªõi memory
    )

    # ‚úÖ Follow-up chain
    follow_up_chain = LLMChain(
        llm=llm,
        prompt=follow_up_prompt,
        output_key="text"
    )

    return main_chain, follow_up_chain, memory


def main():
    st.title("ü§ñ AI Agent Th√¥ng Minh")

    # Kh·ªüi t·∫°o session state n·∫øu ch∆∞a c√≥
    if "main_chain" not in st.session_state:
        with st.spinner("üîÑ ƒêang kh·ªüi t·∫°o..."):
            try:
                documents = load_all_documents("data/")
                vectordb = prepare_vectorstore(documents)
                st.session_state.main_chain, st.session_state.follow_up_chain, st.session_state.memory = build_rag_chain(vectordb)
                st.session_state.conversation = []
                st.success("‚úÖ S·∫µn s√†ng!")
            except Exception as e:
                st.error(f"L·ªói kh·ªüi t·∫°o: {str(e)}")
                st.exception(e)
                return

    if "conversation" not in st.session_state:
        st.session_state.conversation = []

    if "query" not in st.session_state:
        st.session_state.query = ""

    # Hi·ªÉn th·ªã h·ªôi tho·∫°i
    for msg in st.session_state.conversation:
        role = "üë§ Ng∆∞·ªùi d√πng" if msg["role"] == "user" else "ü§ñ AI"
        st.markdown(f"**{role}:** {msg['content']}")

    # Nh·∫≠p c√¢u h·ªèi
    query = st.text_input("üí¨ Nh·∫≠p c√¢u h·ªèi ho·∫∑c g√µ 'ti·∫øp' ƒë·ªÉ nh·∫≠n c√¢u h·ªèi m·ªü r·ªông:", key="query")

    if query:
        with st.spinner("ü§ñ ƒêang x·ª≠ l√Ω..."):

            chat_history_text = "\n".join([
                f"Ng∆∞·ªùi d√πng: {m['content']}" if m["role"] == "user" else f"AI: {m['content']}"
                for m in st.session_state.conversation
            ])

            try:
                if query.lower() == "ti·∫øp":
                    docs = st.session_state.main_chain.retriever.get_relevant_documents("")

                    if docs:
                        context_text = "\n".join([doc.page_content[:1000] for doc in docs[:3]])

                        follow_up_response = st.session_state.follow_up_chain.invoke({
                            "context": context_text,
                            "chat_history": chat_history_text
                        })
                        follow_up_text = follow_up_response.get("text", "B·∫°n mu·ªën bi·∫øt th√™m v·ªÅ ch·ªß ƒë·ªÅ n√†y?")
                        st.session_state.conversation.append({"role": "ai", "content": follow_up_text})
                else:
                    # L∆∞u c√¢u h·ªèi
                    st.session_state.conversation.append({"role": "user", "content": query})
                    st.session_state.memory.chat_memory.add_user_message(query)

                    # Truy v·∫•n h·ªá th·ªëng ch√≠nh
                    response = st.session_state.main_chain({"question": query})
                    answer = response.get("result", response.get("answer", "Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi"))
                    st.session_state.memory.chat_memory.add_ai_message(answer)

                    # Hi·ªÉn th·ªã k·∫øt qu·∫£
                    source_docs = response.get("source_documents", [])
                    sources = [doc.metadata.get('filename', '') for doc in source_docs]

                    st.session_state.conversation.append({
                        "role": "ai",
                        "content": f"{answer}\n\nüîó Ngu·ªìn: {sources}"
                    })

                    # G·ª£i √Ω follow-up n·∫øu c√≥ t√†i li·ªáu
                    if source_docs:
                        context_text = "\n".join([doc.page_content[:1000] for doc in source_docs[:3]])

                        follow_up_response = st.session_state.follow_up_chain.invoke({
                            "context": context_text,
                            "chat_history": chat_history_text + f"\nNg∆∞·ªùi d√πng: {query}\nAI: {answer}"
                        })
                        follow_up_text = follow_up_response.get("text", "B·∫°n mu·ªën bi·∫øt th√™m v·ªÅ ch·ªß ƒë·ªÅ n√†y?")

                        st.session_state.conversation.append({
                            "role": "ai",
                            "content": f"üí° B·∫°n c√≥ mu·ªën h·ªèi ti·∫øp v·ªÅ: '{follow_up_text}'? (G√µ 'ti·∫øp' ƒë·ªÉ xem)"
                        })

            except Exception as e:
                st.error(f"L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")
                st.exception(e)

        # Reset √¥ nh·∫≠p sau khi x·ª≠ l√Ω
        st.session_state.query = ""


if __name__ == "__main__":
    main()