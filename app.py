import streamlit as st
from modules.document_processor import load_all_documents
from modules.vector_store import prepare_vectorstore
from langchain.chains.llm import LLMChain
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from collections import defaultdict
from dotenv import load_dotenv
import os

@st.cache_resource(show_spinner="ğŸ”„ Äang táº£i vÃ  vector hÃ³a tÃ i liá»‡u...")
def load_vectorstore_and_docs():
    docs = load_all_documents("data/")
    vectorstore = prepare_vectorstore(docs)
    return vectorstore, docs

# Khá»Ÿi táº¡o bá»™ nhá»›
def init_memory():
    return ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="result"
    )

def build_llm_chain():
    llm = Ollama(model="mistral")
    
    # load_dotenv()
    
    # llm = OpenAI(
    # base_url="https://models.inference.ai.azure.com",
    # api_key= os.getenv("GITHUB_TOKEN"),
    # model="gpt-4o",
    # temperature=0.1,
    # )
    # openai_api_key = os.getenv("OPENAI_API_KEY")
    # print(f"ğŸ”‘ OPENAI_API_KEY: {openai_api_key}")
    # if not openai_api_key:
    #     st.error("âŒ OPENAI_API_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh trong .env hoáº·c biáº¿n mÃ´i trÆ°á»ng!")
    #     st.stop()

    # llm = ChatOpenAI(
    #     openai_api_key = openai_api_key,
    #     model_name="gpt-3.5-turbo",
    #     temperature=0.3
    # )
    
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template="""
        Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u sau. Náº¿u khÃ´ng cÃ³ thÃ´ng tin phÃ¹ há»£p, báº¡n váº«n cÃ³ thá»ƒ sá»­ dá»¥ng kiáº¿n thá»©c cá»§a mÃ¬nh Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i há»£p lÃ½.
        TÃªn tÃ i liá»‡u Ä‘Æ°á»£c tham chiáº¿u: {filename}
        TÃ i liá»‡u: {context}
        CÃ¢u há»i: {question}
        Tráº£ lá»i:
        """
    )
    return LLMChain(llm=llm, prompt=prompt)

def main():
    st.title("ğŸ¤– Trá»£ lÃ½ AI")

    if "docs" not in st.session_state:
        vectorstore, docs = load_vectorstore_and_docs()
        st.session_state.vectorstore = vectorstore
        st.session_state.docs = docs
        st.session_state.llm_chain = build_llm_chain()
        st.session_state.memory = init_memory()
        st.session_state.conversation = []
        st.session_state.stop_generation = False
        st.success("âœ… Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng!")

    if "conversation" not in st.session_state:
        st.session_state.conversation = []
    if "stop_generation" not in st.session_state:
        st.session_state.stop_generation = False

    # Hiá»ƒn thá»‹ danh sÃ¡ch tÃ i liá»‡u Ä‘Ã£ load
    with st.expander("ğŸ“„ Danh sÃ¡ch tÃ i liá»‡u Ä‘Ã£ load"):
        filenames = sorted(list({
            doc.metadata["filename"]
            for doc in st.session_state.docs
            if "filename" in doc.metadata and doc.metadata["filename"]
        }))
        st.markdown("\n".join(f"- {f}" for f in filenames))

    col1, col2 = st.columns([5, 1])

    with col1:
        selected_file = st.selectbox("ğŸ“‚ Chá»n tÃ i liá»‡u Ä‘á»ƒ tham chiáº¿u:", filenames)
        with st.form("form_input"):
            query = st.text_input("ğŸ’¬ CÃ¢u há»i cá»§a báº¡n:", key="query_input")
            submitted = st.form_submit_button("Gá»­i")

    with col2:
        if st.button("â¹ Stop Generating"):
            st.session_state.stop_generation = True

    if submitted and query:
        with st.spinner("ğŸ¤– Äang xá»­ lÃ½..."):
            try:
                if st.session_state.stop_generation:
                    st.warning("â›” Pháº£n há»“i Ä‘Ã£ bá»‹ dá»«ng trÆ°á»›c khi báº¯t Ä‘áº§u.")
                    st.session_state.stop_generation = False
                    return

                st.session_state.conversation.append({"role": "user", "content": query})
                st.session_state.memory.chat_memory.add_user_message(query)

                matched_docs = [doc.page_content for doc in st.session_state.docs if doc.metadata.get("filename") == selected_file]
                context = f"TÃªn file: {selected_file}" + "".join(matched_docs)

                response = st.session_state.llm_chain.invoke({
                    "filename": selected_file,
                    "context": context,
                    "question": query
                })

                if st.session_state.stop_generation:
                    st.warning("â›” ÄÃ£ dá»«ng pháº£n há»“i sau khi gá»­i.")
                    st.session_state.stop_generation = False
                    return

                answer = response.get("text", "KhÃ´ng cÃ³ cÃ¢u tráº£ lá»i.")
                st.session_state.memory.chat_memory.add_ai_message(answer)
                st.session_state.conversation.append({"role": "ai", "content": answer})

            except Exception as e:
                st.error(f"Lá»—i khi xá»­ lÃ½: {e}")

        st.session_state.pop("query_input", None)
        st.session_state.stop_generation = False

    # Hiá»ƒn thá»‹ há»™i thoáº¡i gáº§n nháº¥t
    for msg in st.session_state.conversation:
        icon = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
        st.markdown(f"**{icon}**: {msg['content']}")

if __name__ == "__main__":
    main()
